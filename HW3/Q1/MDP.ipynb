{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t\n",
    "<p></p>\n",
    "<p></p>\n",
    "<font size=5>\n",
    "<font/>\n",
    "<p></p>\n",
    " <br/>\n",
    "    <br/>\n",
    "<font color=#FF7500>\n",
    "Sharif University of Technology - Department of Computer Engineering\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=blue>\n",
    "Artificial Intelligence\n",
    "</font>\n",
    "<br/>\n",
    "<br/>\n",
    "Spring 2023\n",
    "\n",
    "<div/>\n",
    "\n",
    "<hr/>\n",
    "\t\t<div align=center>\n",
    "\t\t    <font color=red size=6>\n",
    "\t\t\t    <br />\n",
    "Practical Assignment 3 (Markov Decision Process)\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "                <br/>\n",
    "    </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction\n",
    "\n",
    "In this section, we will introduce the concepts of MDP, Q-values, and V-values. These concepts are fundamental to the field of AI and machine learning, as they are used to model **decision-making problems** in various domains such as \"robotics\", \"finance\", and \"healthcare\".\n",
    "\n",
    "MDP stands for Markov Decision Process. It is a mathematical framework for modeling decision-making problems in which the outcomes are partly random and partly under the control of a decision-maker. MDPs are defined by a set of states, a set of actions, a reward function, and a transition function. The goal is to find a policy that maximizes the expected cumulative reward over time.\n",
    "\n",
    "Q-values and V-values are two important concepts in the context of MDPs. A Q-value represents the expected cumulative reward of taking a particular action in a particular state and following a specific policy thereafter. A V-value represents the expected cumulative reward of being in a particular state and following a specific policy thereafter. These values are used to evaluate and improve the policy of an agent in an MDP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: The Basics of MDPs\n",
    "\n",
    "In this section, we will explain the basic components of an MDP.\n",
    "\n",
    "An MDP is defined by \"a set of states\", \"a set of actions\", \"a reward function\", and \"a transition function\". The state space is the set of all possible states that the agent can be in. The action space is the set of all possible actions that the agent can take. The reward function defines the reward the agent receives for each action taken in a particular state. The transition function defines the probability of moving from one state to another state after taking a particular action.\n",
    "\n",
    "To illustrate these concepts, let's consider an example of a **robot that needs to navigate through a maze**. The robot can be in one of several states, such as at the start of the maze, at a junction in the maze, or at the end of the maze. This robot takes an action. With Probability of **0.8** It goes in that desired direction but with probability of **0.2** It goes in the perpendicular direction (0.1, 0.1 for each)!\n",
    "\n",
    "In an MDP, the agent interacts with the environment by selecting actions based on its current state and the expected future reward. The goal of the agent is to find a policy that maximizes the expected cumulative reward over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**\n",
    "\n",
    "1. What are the state space, action space, reward function, and transition function of the robot in the maze example? Explain why you think each of these components is important for the robot to navigate through the maze.\n",
    "\n",
    "2. Is our environment stochastic or deterministic? Why?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**ANSWER**\n",
    "1. state space includes all the possible block that can be reached\n",
    "   action space includes the 4 possible moves (N, W, S,  E)\n",
    "   reward function will be implemented soon\n",
    "   transition function: by probability of 80 percent it follows our policy and by probability of 10 percent it goes up(same thing happens for going down)\n",
    "\n",
    "2. we have uncertainty so our environment is deterministic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define The MDP**:\n",
    "\n",
    "Based on your choice of rewards and transitions and the state space, define the MDP for the robot in the maze example. You can complete the following code to define the MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Maze is: \n",
      " [[2 0 0 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 1 1 1 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definition of the maze\n",
    "maze = np.array([[2, 0, 0, 0, 0],\n",
    "                 [0, 1, 1, 0, 1],\n",
    "                 [0, 0, 0, 0, 0],\n",
    "                 [0, 1, 1, 1, 3]])\n",
    "\n",
    "print(\"Our Maze is: \\n\", maze)\n",
    "\n",
    "# TODO: Define the states and actions\n",
    "states = []\n",
    "for i in range(4):\n",
    "    tmp = []\n",
    "    for j in range(5):\n",
    "        tmp.append((i, j))\n",
    "    states.append(tmp)\n",
    "\n",
    "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "\n",
    "# TODO: Define the reward function\n",
    "rewards = [[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0]]\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        if maze[i][j] == 1:\n",
    "            rewards[i][j] = -1.0\n",
    "        elif maze[i][j] == 3:\n",
    "            rewards[i][j] = 1.0\n",
    "\n",
    "\n",
    "# TODO: Define the transition probabilities\n",
    "transition_probs = [0.8, 0.1, 0.1]  # desired, up, down\n",
    "\n",
    "# TODO: Set the discount factor (for further use in v-value iteration and q-value iteration)\n",
    "discount = 0.9\n",
    "\n",
    "# TODO: Define the initial value function (you can simply set all to 0)\n",
    "values = [[0.0 for _ in range(5)] for _ in range(4)]\n",
    "\n",
    "# Define the initial Q function (you can simply set all to 0)\n",
    "q_values = [[0.0 for _ in range(4)] for _ in range(20)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Computing V-values\n",
    "\n",
    "In this section, we will explain how to compute V-values for an MDP using the Bellman equation.\n",
    "\n",
    "The Bellman equation is a recursive equation that expresses the value of a state in terms of the values of its successor states. It is defined as:\n",
    "\n",
    "$$V(s) = R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n",
    "\n",
    "where V(s) is the value of state s, R(s) is the reward for being in state s, γ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, and max_a is the maximum over all possible actions a.\n",
    "\n",
    "To compute the V-values for an MDP, we start with an initial estimate of the V-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n",
    "\n",
    "$$V(s) \\leftarrow R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n",
    "\n",
    "We can use dynamic programming algorithms such as value iteration or policy iteration to compute the V-values.\n",
    "\n",
    "We can use the Bellman equation to compute the V-values for each state in the maze. The V-values represent the expected cumulative reward that the robot can obtain if it starts from that state and follows an optimal policy thereafter. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1873071620439344, 1.3464023931959859, 1.8111805033620951, 2.3405111288290525, 1.8111805033620951]\n",
      "[1.457345083283926, 1.710163119318918, 2.3711840838973437, 2.7529199101659723, 3.2099248499537496]\n",
      "[1.6672684428394715, 1.9617122270756533, 2.511950036391716, 3.1386097636683425, 3.761627906976742]\n",
      "[1.3741620205494196, 1.6362493449794275, 2.126021460964055, 3.6418711930844982, 3.2383720930232536]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the V-values using Bellman equations\n",
    "def get_possible_actions(current_state):\n",
    "    tmp = []\n",
    "    for action in actions:\n",
    "        x = current_state[0] + action[0]\n",
    "        y = current_state[1] + action[1]\n",
    "        if 0 <= x <= 3 and 0 <= y <= 4 and maze[x][y] != 1:\n",
    "            tmp.append((action, False))\n",
    "        else:\n",
    "            tmp.append((action, True))\n",
    "    return tmp\n",
    "\n",
    "\n",
    "tmp_values = [[0.0 for _ in range(5)] for _ in range(4)]\n",
    "\n",
    "\n",
    "def value_compute():\n",
    "    for i in range(4):\n",
    "        for j in range(5):\n",
    "            temp = []\n",
    "            current_state = states[i][j]\n",
    "            possible_actions = get_possible_actions(current_state)\n",
    "            for action in possible_actions:\n",
    "                move = action[0]\n",
    "                has_collision = action[1]\n",
    "                if move == (0, 1) or move == (0, -1):\n",
    "                    val = 0.0\n",
    "                    if has_collision:\n",
    "                        val += transition_probs[0] * (values[current_state[0]][current_state[1]] * discount - 1.0)\n",
    "\n",
    "                    else:\n",
    "                        val += transition_probs[0] * (values[current_state[0]][current_state[1] + move[1]] * discount + rewards[current_state[0]][current_state[1] + move[1]])\n",
    "                    if possible_actions.count(((1, 0), True)):\n",
    "                        val += transition_probs[1] * (values[current_state[0]][current_state[1]] * discount - 1.0)\n",
    "                    if possible_actions.count(((-1, 0), True)):\n",
    "                        val += transition_probs[2] * (values[current_state[0]][current_state[1]] * discount - 1.0)\n",
    "                    if possible_actions.count(((1, 0), False)):\n",
    "                        val += transition_probs[1] * (values[current_state[0] + 1][current_state[1]] * discount + rewards[current_state[0] + 1][current_state[1]])\n",
    "                    if possible_actions.count(((-1, 0), False)):\n",
    "                        val += transition_probs[1] * (values[current_state[0] - 1][current_state[1]] * discount + rewards[current_state[0] - 1][current_state[1]])\n",
    "                    temp.append(val)\n",
    "\n",
    "                elif move == (1, 0):\n",
    "                    val = 0.0\n",
    "                    if has_collision:\n",
    "                        val += (transition_probs[0] + transition_probs[1]) * (values[current_state[0]][current_state[1]] * discount - 1.0)\n",
    "                    else:\n",
    "                        val += (transition_probs[0] + transition_probs[1]) * (values[current_state[0] + 1][current_state[1]] * discount + rewards[current_state[0] + 1][current_state[1]])\n",
    "                    if possible_actions.count(((-1, 0), True)):\n",
    "                        val += transition_probs[2] * (values[current_state[0]][current_state[1]] * discount - 1.0)\n",
    "                    if possible_actions.count(((-1, 0), False)):\n",
    "                        val += transition_probs[2] * (values[current_state[0] - 1][current_state[1]] * discount + rewards[current_state[0] - 1][current_state[1]])\n",
    "                    temp.append(val)\n",
    "\n",
    "                else:\n",
    "                    val = 0.0\n",
    "                    if has_collision:\n",
    "                        val += (transition_probs[0] + transition_probs[2]) * (values[current_state[0]][current_state[1]] * discount - 1.0)\n",
    "                    else:\n",
    "                        val += (transition_probs[0] + transition_probs[1]) * (values[current_state[0] - 1][current_state[1]] * discount + rewards[current_state[0] - 1][current_state[1]])\n",
    "                    if possible_actions.count(((1, 0), True)):\n",
    "                        val += transition_probs[2] * (values[current_state[0]][current_state[1]] * discount - 1.0)\n",
    "                    if possible_actions.count(((1, 0), False)):\n",
    "                        val += transition_probs[2] * (values[current_state[0] + 1][current_state[1]] * discount + rewards[current_state[0] + 1][current_state[1]])\n",
    "                    temp.append(val)\n",
    "            tmp_values[i][j] = max(temp)\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    value_compute()\n",
    "    values = tmp_values[:]\n",
    "\n",
    "# Print the V-values\n",
    "for i in range(4):\n",
    "    print(values[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Computing Q-values\n",
    "\n",
    "In this section, we will explain how to compute Q-values for an MDP using the Bellman equation.\n",
    "\n",
    "The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. The Q-values can be computed using the Bellman equation as follows:\n",
    "\n",
    "$$Q(s, a) = R(s, a) + \\gamma * \\sum_{s'} (P(s, a, s') * \\max_{a'} (Q(s', a')))$$\n",
    "\n",
    "where Q(s, a) is the Q-value of state-action pair (s, a), R(s, a) is the reward for taking action a in state s, γ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, max_a' is the maximum over all possible actions a' in state s', and sum_s' is the sum over all possible successor states s' of state s.\n",
    "\n",
    "To compute the Q-values for an MDP, we start with an initial estimate of the Q-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n",
    "\n",
    "$$Q(s, a) \\leftarrow R(s, a) + \\gamma * \\sum_{s'} (P(s, a, s') * \\max_{a'} (Q(s', a')))$$\n",
    "\n",
    "We can use dynamic programming algorithms such as Q-learning or SARSA to compute the Q-values.\n",
    "\n",
    "\n",
    "We can use the Q-learning algorithm to compute the Q-values for each state-action pair in the maze. The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.1074284251806175, 0.1928798587511403, 1.1873071620439344, 0.1928798587511403]\n",
      "[1.3464023931959859, 0.8972135874469103, 0.21176215387638744, 0.21176215387638744]\n",
      "[1.8111805033620951, 1.095422213706287, 0.6300624530258856, 0.6300624530258856]\n",
      "[1.6624587559302606, 1.6624587559302606, 2.3405111288290525, 1.24357680626647]\n",
      "[0.6300624530258857, 1.8111805033620951, 0.6300624530258856, 0.6300624530258856]\n",
      "[0.5062002644039334, 0.5062002644039334, 1.457345083283926, 1.1117729611111393]\n",
      "[0.7290477617340686, 1.3470187757888745, 1.710163119318918, 1.2671400389255574]\n",
      "[2.3711840838973437, 1.2963342889839304, 2.197685774779879, 1.6931317109985515]\n",
      "[1.675223215644266, 1.675223215644266, 2.7529199101659723, 2.1782888930816835]\n",
      "[2.012698648897195, 2.483655092249996, 3.2099248499537496, 1.8056027193512039]\n",
      "[1.6672684428394715, 0.6552689181894207, 1.2442322941405832, 1.304124099309428]\n",
      "[1.9617122270756533, 1.3535414797180374, 0.7655410043680879, 0.7655410043680879]\n",
      "[2.511950036391716, 1.6645838100449795, 1.2607550327525445, 1.2607550327525445]\n",
      "[3.1386097636683425, 2.238841696847124, 1.890036700486295, 2.4123400059645888]\n",
      "[2.538372093023254, 2.8897990298412064, 3.761627906976742, 2.538372093023254]\n",
      "[0.3631253965005824, 0.3631253965005824, 0.3631253965005824, 1.3741620205494196]\n",
      "[0.6019160698701451, 1.2132131962805395, 0.6019160698701451, 1.6362493449794275]\n",
      "[1.0481528866561391, 1.0481528866561391, 1.0481528866561391, 2.126021460964055]\n",
      "[3.6418711930844982, 2.332390545128595, 2.332390545128595, 2.770042315948962]\n",
      "[2.0616279069767423, 2.0616279069767423, 2.0616279069767423, 3.2383720930232536]\n"
     ]
    }
   ],
   "source": [
    "temp_qs = q_values.copy()\n",
    "\n",
    "def q_compute():\n",
    "    for i in range(4):\n",
    "        for j in range(5):\n",
    "            current_state = states[i][j]\n",
    "            possible_actions = get_possible_actions(current_state)\n",
    "            for k in range(len(possible_actions)):\n",
    "                action = possible_actions[k]\n",
    "                move = action[0]\n",
    "                has_collision = action[1]\n",
    "                if move == (0, 1) or move == (0, -1):\n",
    "                    val = 0.0\n",
    "                    if has_collision:\n",
    "                        val += transition_probs[0] * (-1.0 + discount * max(q_values[current_state[0] * 5 + current_state[1]]))\n",
    "                    else:\n",
    "                        val += transition_probs[0] * (rewards[current_state[0]][current_state[1] + move[1]] + discount * max(q_values[current_state[0] * 5 + current_state[1] + move[1]]))\n",
    "                    if possible_actions.count(((1, 0), True)):\n",
    "                        val += transition_probs[1] * (-1.0 + discount * max(q_values[current_state[0] * 5 + current_state[1]]))\n",
    "                    if possible_actions.count(((-1, 0), True)):\n",
    "                        val += transition_probs[2] * (-1.0 + discount * max(q_values[current_state[0] * 5 + current_state[1]]))\n",
    "                    if possible_actions.count(((1, 0), False)):\n",
    "                        val += transition_probs[1] * (rewards[current_state[0] + 1][current_state[1]] + discount * max(q_values[(current_state[0] + 1) * 5 + current_state[1]]))\n",
    "                    if possible_actions.count(((-1, 0), False)):\n",
    "                        val += transition_probs[1] * (rewards[current_state[0] - 1][current_state[1]] + discount * max(q_values[(current_state[0] - 1) * 5 + current_state[1]]))\n",
    "                    if move == (0, 1):\n",
    "                        temp_qs[current_state[0] * 5 + current_state[1]][0] = val\n",
    "                    else:\n",
    "                        temp_qs[current_state[0] * 5 + current_state[1]][1] = val\n",
    "\n",
    "                elif move == (1, 0):\n",
    "                    val = 0.0\n",
    "                    if has_collision:\n",
    "                        val += (transition_probs[0] + transition_probs[1]) * (-1.0 + discount * max(q_values[current_state[0] * 5 + current_state[1]]))\n",
    "                    else:\n",
    "                        val += (transition_probs[0] + transition_probs[1]) * (rewards[current_state[0] + move[0]][current_state[1]] + discount * max(q_values[(current_state[0] + 1) * 5 + current_state[1]]))\n",
    "                    if possible_actions.count(((-1, 0), True)):\n",
    "                        val += transition_probs[2] * (-1.0 + discount * max(q_values[current_state[0] * 5 + current_state[1]]))\n",
    "                    if possible_actions.count(((-1, 0), False)):\n",
    "                        val += transition_probs[1] * (rewards[current_state[0]- 1][current_state[1]] + discount * max(q_values[(current_state[0] - 1) * 5 + current_state[1]]))\n",
    "                    temp_qs[current_state[0] * 5 + current_state[1]][2] = val\n",
    "\n",
    "                else:\n",
    "                    val = 0.0\n",
    "                    if has_collision:\n",
    "                        val  += (transition_probs[0] + transition_probs[2]) * (-1.0 + discount * max(q_values[current_state[0] * 5 + current_state[1]]))\n",
    "                    else:\n",
    "                        val += (transition_probs[0] + transition_probs[2]) * (rewards[current_state[0] + move[0]][current_state[1]] + discount * max(q_values[(current_state[0] - 1) * 5 + current_state[1]]))\n",
    "                    if possible_actions.count(((1, 0), True)):\n",
    "                        val += transition_probs[1] * (-1.0 + discount * max(q_values[current_state[0] * 5 + current_state[1]]))\n",
    "                    if possible_actions.count(((1, 0), False)):\n",
    "                        val += transition_probs[1] * (rewards[current_state[0] + 1][current_state[1]] + discount * max(q_values[(current_state[0] + 1) * 5 + current_state[1]]))\n",
    "                    temp_qs[current_state[0] * 5 + current_state[1]][3] = val\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    q_compute()\n",
    "    q_values = temp_qs.copy()\n",
    "\n",
    "# Print the Q-values\n",
    "print()\n",
    "for i in range(20):\n",
    "    print(q_values[i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Visualizing the Optimal Policy\n",
    "\n",
    "Now that we have computed the Q-values, we can use them to find the optimal policy, which is the sequence of actions that the robot should take in each state to maximize its expected reward. We can visualize the optimal policy as arrows in a grid, where each arrow corresponds to the action with the highest Q-value in the corresponding state. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
      "[2, 0, 0, 2, 1]\n",
      "[2, 2, 0, 2, 2]\n",
      "[0, 0, 0, 0, 2]\n",
      "[3, 3, 3, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the optimal policy\n",
    "\n",
    "policy = [[0, 0, 0, 0, 0],\n",
    "          [0, 0, 0, 0, 0],\n",
    "          [0, 0, 0, 0, 0],\n",
    "          [0, 0, 0, 0, 0]]\n",
    "\n",
    "for i in range(20):\n",
    "    x = i // 5\n",
    "    y = i % 5\n",
    "    policy[x][y] = q_values[i].index(max(q_values[i]))\n",
    "\n",
    "print(actions)\n",
    "for i in range(4):\n",
    "    print(policy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Visualize optimal policy as arrows\n",
    "actions_signs = ['>', '<', 'v', '^']\n",
    "\n",
    "\n",
    "def policy_plot():\n",
    "    row = 4\n",
    "    col = 5\n",
    "    policy_symbolic = [\"\" for _ in range(col * row)]\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            policy_symbolic[i * 5 + j] = actions_signs[policy[i][j]]\n",
    "    plt.figure()\n",
    "    cnt = 0\n",
    "    for rr in range(row, 0, -1):\n",
    "        for cc in range(1, col + 1):\n",
    "            plt.fill([cc - 1, cc - 1, cc,  cc], [rr, rr - 1, rr - 1, rr], 'b')\n",
    "            plt.text(cc - 0.55, rr - 0.5, policy_symbolic[cnt])\n",
    "            cnt += 1\n",
    "    plt.show()\n",
    "\n",
    "policy_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
